---
title: 'Autonomous AI Fixes Quickstart'
sidebarTitle: 'Quickstart'
---

import { Callout } from "nextra/components"
import { Steps } from "nextra/components"

# Autonomous AI Fixes Quickstart

Set up your autonomous engineer in under 10 minutes. This guide shows you how to enable automated issue detection and fix generation that creates pull requests with proven improvements.

<Callout type="info">
  **Prerequisites**: You need active Handit.ai evaluation running on your LLM nodes. If you haven't set up evaluation yet, start with our [Main Quickstart](/quickstart).
</Callout>

## Quick Setup with CLI & GitHub (Recommended)

### Step 1: Set Up GitHub Integration

```bash filename="terminal"
handit-cli github
```

The CLI will guide you through:
- **Installing the Handit GitHub app** on your repository
- **Configuring repository permissions** for automated PRs
- **Setting up optimization models** if not already configured
- **Enabling automated pull request creation** for optimizations

<Callout type="tip">
  **Why GitHub Integration?** Your autonomous engineer creates pull requests with fixes when it detects quality issues. You review and merge the improvements you wantâ€”just like working with a human engineer.
</Callout>

### Step 2: Monitor and Deploy

**Receive Automated Pull Requests:**
- Your autonomous engineer creates PRs when fixes are ready
- Review the exact changes to prompts and system configurations
- Merge approved improvements through your normal workflow

**Monitor in Dashboard:**
- View autonomous engineer progress in **Agent Performance**
- See detailed fix comparisons in **Release Hub**

<Callout type="success">
  **Your autonomous engineer is now active!** It will automatically detect quality issues, generate fixes, test them, and create pull requests with proven improvementsâ€”24/7.
</Callout>

## Manual Setup (Alternative)

If you prefer manual control over deployments, follow these steps:

<Steps>
### Connect Optimization Models
Add AI models that will analyze problems and generate improved prompts

### Monitor Optimization Results  
View performance comparisons between current and optimized prompts

### Deploy via Release Hub
Mark winning prompts as production and fetch them via SDK
</Steps>

## Step 1: Connect Optimization Models

Connect the AI models that will analyze your evaluation data and generate optimized prompts.

**1. Navigate to Optimization Settings**
- Go to your Handit.ai dashboard
- Click **Optimization** â†’ **Settings** â†’ **Model Tokens**
- Click **Add Optimization Token** or select an existing token

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/model_token.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

**2. Configure Your Optimization Model**

Select or configure your optimization model:

**3. Save Configuration**
- Save the optimization token configuration
- Self-improving AI automatically activates

<Callout type="tip">
  **Automatic Activation**: Once optimization tokens are configured, self-improving AI automatically begins analyzing your evaluation data and generating optimizations. No additional setup required!
</Callout>

## Step 2: Monitor Optimization Results

Your AI is now automatically generating and testing improved prompts. Monitor the results in two places:

**1. Agent Performance Dashboard**
- Go to **Agent Performance** 
- View how your agents and LLM nodes are performing
- Compare current vs optimized versions

![Agent Performance Dashboard](/assets/overview/general-handit.png)

**Example Metrics:**
```
Customer Support Agent

LLM Node: Response Generator
â”œâ”€â”€ Current Version: 4.2/5.0 overall quality
â”œâ”€â”€ Optimized Version: 4.6/5.0 overall quality
â””â”€â”€ Improvement: +9.5% quality increase

Recent Optimizations:
âœ… Empathy Enhancement: +15% empathy score
âœ… Technical Clarity: +12% accuracy score  
ðŸ”„ Response Completeness: Currently testing
```

**2. Release Hub Analysis**
- Go to **Optimization** â†’ **Release Hub**
- View detailed metrics for each prompt per LLM node
- See performance comparisons and deployment recommendations

![Release Hub - Prompt Performance Comparison](/assets/overview/prompt-comparison.png)

**Example Release Hub View:**
```
Customer Support LLM - Prompt Performance

Current Production Prompt:
- Overall Quality: 4.2/5.0
- Empathy: 4.0/5.0
- Accuracy: 4.3/5.0

Optimized Prompt v1.2:
- Overall Quality: 4.6/5.0 (+9.5% improvement)
- Empathy: 4.7/5.0 (+17.5% improvement)
- Accuracy: 4.5/5.0 (+4.7% improvement)

Statistical Confidence: 95%
Recommendation: Ready for production
```

## Step 3: Deploy Optimizations

Select winning optimizations in the Release Hub and integrate them into your applications via SDK.

**1. Navigate to Release Hub**
- Go to **Optimization** â†’ **Release Hub**
- View recommended optimizations for each LLM node

**2. Compare and Select**
```
Recommended for Production:

âœ… Empathy Enhancement v1.2
Performance: +15% empathy, +7% overall quality
Status: Recommended for production

âœ… Technical Accuracy v2.1  
Performance: +12% accuracy, +5% overall quality
Status: Ready for production
```

**3. Mark as Production**
- Select the optimization you want to use
- Click **Mark as Production**
- Prompt becomes immediately available via SDK

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/ci:cd.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

**4. Fetch via SDK**

Once marked as production, fetch optimized prompts directly in your application:

**Python SDK:**
```python
from handit import HanditTracker

# Initialize with your project
tracker = HanditTracker(api_key="your-api-key")

# Fetch the current production prompt
optimized_prompt = tracker.fetch_optimized_prompt(model_id="customer-support-llm")

# Use in your own LLM calls
response = your_llm_client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": optimized_prompt},
        {"role": "user", "content": user_query}
    ]
)
```

**JavaScript SDK:**
```javascript
import { HanditClient } from '@handit/sdk';

// Initialize client
const handit = new HanditClient({ apiKey: 'your-api-key' });

// Fetch current production prompt
const optimizedPrompt = await handit.fetchOptimizedPrompt({ 
  modelId: 'customer-support-llm' 
});

// Use in your own LLM calls
const response = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: [
    { role: 'system', content: optimizedPrompt },
    { role: 'user', content: userQuery }
  ]
});
```

**5. Monitor Performance**
- Track the performance of deployed prompts in Agent Performance
- Monitor for any issues in your applications
- Continue the optimization cycle

## Next Steps

- Explore [Advanced Optimization Features](/optimization/optimization_features/ab_testing)
- Set up [CI/CD Deployment](/optimization/optimization_features/cicd_deployment)
- Configure [Release Hub](/optimization/guide/release_hub)
- Visit [GitHub Issues](https://github.com/Handit-AI/handit.ai-docs/issues) for assistance

<Callout type="success">
  **Congratulations!** You now have a self-improving AI system with GitHub integration that automatically detects issues, generates solutions, tests improvements, and creates pull requests with proven optimizations.
</Callout>


## Troubleshooting

**CLI GitHub Setup Issues?**
- Ensure you have admin access to the repository
- Try running `handit-cli github` again to reinstall the app
- Check repository permissions in your GitHub settings
- Verify the GitHub app is installed on the correct repository

**Pull Requests Not Being Created?**
- Confirm the GitHub integration is properly configured
- Check that optimizations are being generated (visible in Release Hub)
- Verify repository permissions allow PR creation
- Ensure the target branch exists and is accessible

**Optimizations Not Generating?**
- Check that evaluation data shows quality issues (scores below threshold)
- Verify optimization model tokens are configured (done during `handit-cli setup`)
- Ensure evaluation has been running long enough to collect data
- Confirm optimization is enabled for the correct LLM nodes

**Need Help?**
- Check our [detailed optimization guides](/optimization/guide/cli_github_setup)
- Visit [Support](/more/contact) for assistance
- Join our [Discord community](https://discord.gg/wZbW9Bu5) 