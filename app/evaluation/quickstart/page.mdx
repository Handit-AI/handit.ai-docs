---
title: 'Evaluation Quickstart'
sidebarTitle: 'Quickstart'
---

import { Callout } from "nextra/components"
import { Steps } from "nextra/components"

# Evaluation Quickstart

> **Give your autonomous engineer the data it needs to detect issues and create fixes.** This guide walks you through setting up comprehensive quality assessment that powers autonomous optimization.

<Callout type="info">
  **Already set up Handit?** If you followed our [Main Quickstart](/quickstart), evaluation is already working. This page explains how to view and understand your evaluation data.
</Callout>

## Quick Setup with CLI (Recommended)

### Step 1: Run Evaluators Setup

```bash filename="terminal"
handit-cli evaluators-setup
```

The CLI will guide you through:
- **Connecting evaluation models** (OpenAI, Together AI, etc.)
- **Adding AI model tokens** for evaluation
- **Connecting existing evaluators** to your LLM nodes
- **Setting evaluation percentages** and priorities

<Callout type="success">
  **That's it!** Your evaluation system is now active and will start assessing your AI outputs automatically.
</Callout>

### Step 2: Monitor Evaluation Results

View real-time evaluation results in your dashboard:

**Real-Time Monitoring:**
- Go to **Tracing** tab to see individual evaluation scores
- Monitor quality trends in **Agent Performance** dashboard

![AI Agent Tracing Dashboard](/assets/overview/tracing.png)

**Quality Analytics:**
- Access **Agent Performance** for trends across different quality dimensions
- Understand which aspects are performing well or need attention

![Agent Performance Dashboard](/assets/overview/general-handit.png)

## Manual Setup (Advanced)

If you prefer manual configuration or need custom setups, follow these detailed steps:

<Steps>
### Connect Model Tokens
Add your evaluation models (GPT-4, Llama, etc.) to the platform

### Create Single-Purpose Evaluators  
Design focused evaluators that check ONE specific quality aspect

### Associate Evaluators to LLM Nodes
Connect your evaluators to the AI functions you want to assess

### Monitor Results
View evaluation results and quality insights in real-time
</Steps>

<Callout type="warning">
  **Critical Best Practice**: Create separate evaluators for each quality aspect (completeness, accuracy, format, etc.). Don't try to evaluate multiple things in one prompt—this reduces effectiveness and clarity.
</Callout>

## Step 1: Connect Model Tokens

Connect the AI models that will act as "judges" to evaluate your LLM responses.

### Navigate to Model Tokens
- Go to your Handit.ai dashboard
- Click **Settings** → **Model Tokens**
- Click **Add New Token**

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/model_token.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

Add your OpenAI or Together AI credentials to connect evaluation models.

<Callout type="success">
  **Recommended Models**: GPT-4o (highest accuracy), GPT-3.5-turbo (cost-effective), Llama v4 Scout (open source alternative)
</Callout>

## Step 2: Create Single-Purpose Evaluators

Create focused evaluators in the Evaluation Suite. Remember: **one evaluator = one quality aspect**.

### Navigate to Evaluation Suite
- Go to **Evaluation** → **Evaluation Suite**
- Click **Create New Evaluator**

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/evaluator_creation.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

### Example 1: Completeness Evaluator

**Evaluation Prompt:**
```
You are evaluating whether an AI response completely addresses the user's question.

Focus ONLY on completeness - ignore other quality aspects.

User Question: {input}
AI Response: {output}

Rate on a scale of 1-10:
1-2 = Missing major parts of the question
3-4 = Addresses some parts but incomplete
5-6 = Addresses most parts adequately  
7-8 = Addresses all parts well
9-10 = Thoroughly addresses every aspect

Provide your score and brief reasoning.

Output format:
Score: [1-10]
Reasoning: [Brief explanation]
```

### Example 2: Hallucination Detection Evaluator

**Evaluation Prompt:**
```
You are checking if an AI response contains hallucinations or made-up information.

Focus ONLY on factual accuracy - ignore other aspects like tone or completeness.

User Question: {input}
AI Response: {output}

Rate on a scale of 1-10:
1-2 = Contains obvious false information
3-4 = Contains questionable claims
5-6 = Mostly accurate with minor concerns
7-8 = Accurate information
9-10 = Completely accurate and verifiable

Output format:
Score: [1-10]
Reasoning: [Brief explanation of any concerns]
```

### Example 3: Format Compliance Evaluator

**Evaluation Prompt:**
```
You are checking if an AI response follows the required format guidelines.

Focus ONLY on format compliance - ignore content quality.

User Question: {input}
AI Response: {output}
Expected Format: [Professional tone, structured paragraphs, proper punctuation]

Rate on a scale of 1-10:
1-2 = Poor formatting, unprofessional
3-4 = Some format issues
5-6 = Acceptable formatting
7-8 = Good formatting
9-10 = Perfect format compliance

Output format:
Score: [1-10]
Reasoning: [Brief format assessment]
```

<Callout type="info">
  **Why Separate Evaluators?** Each evaluator focuses on one aspect, making evaluation more reliable and providing clear insights. You can see exactly which quality dimensions are performing well or need attention.
</Callout>

## Step 3: Associate Evaluators to LLM Nodes

Connect your evaluators to the specific LLM nodes you want to monitor.

### Find Your LLM Nodes
- Go to **Tracing** → **Nodes**
- Identify the LLM nodes you want to evaluate (e.g., "customer-response-generation")

### Associate Evaluators
- Click on your target LLM node
- Navigate to **Evaluation** tab
- Click **Add Evaluator**

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/associate_evaluator.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

### Configure Evaluation Settings

For each evaluator you want to associate:

```
Evaluator: "Response Completeness Check"
Evaluation Percentage: 10% (start small)
Priority: Normal
```

```
Evaluator: "Hallucination Detection"  
Evaluation Percentage: 15% (higher for critical accuracy)
Priority: High
```

```
Evaluator: "Format Compliance"
Evaluation Percentage: 5% (lower for less critical aspects)
Priority: Normal
```

<Callout type="tip">
  **Sampling Strategy**: Start with 5-15% evaluation percentage. You can always increase it later. Use higher percentages for critical quality aspects.
</Callout>

### Save Configuration
- Review your evaluator associations
- Click **Save** to activate evaluation

## Step 4: Monitor Evaluation Results

**Your evaluation system is now active!** Here's how to monitor the results.

### Real-Time Monitoring
- Go to **Tracing** tab in your dashboard
- View evaluation scores as they happen
- Monitor quality trends across different aspects

![AI Agent Tracing Dashboard](/assets/overview/tracing.png)

### Quality Analytics
- Access **Agent Performance** tab in your dashboard
- View trends for each agent with their associated evaluators
- Understand which quality aspects are performing well or need attention for each agent

![Agent Performance Dashboard](/assets/overview/general-handit.png)

### Sample Result Display

## Additional Evaluator Examples

### Customer Service Evaluators

**Empathy Evaluator:**
```
Focus: Does the response show understanding and care for the customer's situation?
Scale: 1-10 (1-2=cold/robotic, 9-10=highly empathetic)
```

**Solution Clarity Evaluator:**
```
Focus: Are the solution steps clear and easy to follow?
Scale: 1-10 (1-2=confusing steps, 9-10=crystal clear instructions)
```

**Policy Compliance Evaluator:**
```
Focus: Does the response follow company policies and guidelines?
Scale: 1-10 (1-2=policy violations, 9-10=perfect compliance)
```

### Technical Support Evaluators

**Technical Accuracy Evaluator:**
```
Focus: Is the technical information provided correct?
Scale: 1-10 (1-2=incorrect technical details, 9-10=completely accurate)
```

**Safety Check Evaluator:**
```
Focus: Are the suggested actions safe and won't cause system damage?
Scale: 1-10 (1-2=potentially dangerous, 9-10=completely safe)
```

**Troubleshooting Flow Evaluator:**
```
Focus: Does the response follow logical troubleshooting steps?
Scale: 1-10 (1-2=illogical flow, 9-10=perfect troubleshooting sequence)
```

## Best Practices

<div className="grid grid-cols-2 gap-4 mt-4">
  <div>
    <h4>✅ Do's</h4>
    - Create one evaluator per quality aspect
    - Use clear, specific evaluation criteria
    - Start with lower evaluation percentages
    - Focus on business-critical quality dimensions
    - Test evaluators before deploying
  </div>
  <div>
    <h4>❌ Don'ts</h4>
    - Combine multiple quality checks in one evaluator
    - Use vague evaluation criteria
    - Evaluate 100% of traffic initially
    - Ignore the results without taking action
    - Create overly complex evaluation prompts
  </div>
</div>

## Next Steps

- Set up [Optimization](/optimization) to automatically improve your prompts
- Explore [Advanced Evaluation Features](/evaluation/evaluation_features/llm_as_judges)
- Configure [Custom Evaluators](/evaluation/evaluation_features/custom_evaluators)
- Visit [GitHub Issues](https://github.com/Handit-AI/handit.ai-docs/issues) for assistance

<Callout type="success">
  **Congratulations!** You now have automated AI evaluation running on your production data in under 5 minutes using the CLI. Each evaluator will help you understand specific quality aspects. This evaluation data provides the foundation for AI optimization (covered in our [Autonomous AI Fixes Quickstart](/optimization/quickstart)).
</Callout>

## Troubleshooting

**CLI Setup Issues?**
- Re-run `handit-cli evaluators-setup` to reconfigure
- Ensure you have valid model API keys ready
- Check that your Handit.ai account has access to evaluation features

**Evaluations Not Running?**
- Verify model tokens were properly configured during CLI setup
- Check your model tokens have sufficient credits
- Ensure the LLM node is actively receiving traffic
- Confirm evaluation percentage is > 0%

**Inconsistent Evaluation Scores?**
- Review evaluator prompts for clarity and specificity
- Consider if evaluation criteria are too subjective
- Test evaluators with known good/bad examples
- Focus on single quality aspects per evaluator

## Next Steps

Your autonomous engineer now has the evaluation data it needs to detect issues and create fixes:

- **Enable Autonomous Fixes**: Set up [GitHub Integration](/optimization/quickstart) to complete your autonomous engineer
- **Custom Quality Checks**: Create [Custom Evaluators](/evaluation/guide/custom_evaluators) for specialized quality assessment
- **Advanced Features**: Explore [Evaluation Features](/evaluation/evaluation_features/overview) for comprehensive monitoring

<Callout type="success">
  **Your autonomous engineer can now see quality issues!** Next, enable autonomous fixes so it can automatically create PRs to resolve problems.
</Callout>

**Need Help?**
- Check our [detailed evaluation guides](/evaluation/evaluation_features/overview)
- Visit [Support](/more/contact) for assistance
- Join our [Discord community](https://discord.gg/wZbW9Bu5) 