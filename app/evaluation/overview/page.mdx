---
title: Handit.ai Evaluation
description: Automated AI model evaluation and quality assessment with LLM-as-Judge technology.
sidebarTitle: Overview
---

import { Callout } from "nextra/components";
import { Cards } from 'nextra/components'

# Evaluation

> **Your autonomous engineer needs to know what's broken to fix it.** Handit.ai's evaluation system provides comprehensive quality assessment that powers autonomous issue detection and fix generation.

Transform your AI quality control from reactive spot-checking to autonomous monitoring and fixing.

## The Manual Review Challenge

Picture this: Your AI chatbot handles thousands of customer inquiries daily. You suspect quality is declining, but manually reviewing responses is overwhelming. You randomly check 50 conversations and find issues‚Äîbut what about the other 4,950 interactions?

**Manual evaluation doesn't scale.** You need:
- **Consistency** - Different reviewers have different standards
- **Coverage** - You can't manually check every interaction  
- **Speed** - By the time you review, the damage is done
- **Objectivity** - Human reviewers bring unconscious bias
- **Specificity** - Understanding exactly what needs improvement

<Callout type="info">
  Your autonomous engineer solves this by using AI to evaluate AI‚Äîat scale, consistently, and with focused insights. This evaluation data powers automatic issue detection and fix generation.
</Callout>

## Why AI Evaluation Matters

<details>
<summary>**Scale Quality Control Across Your AI Systems**</summary>

- **Automated assessment** - Evaluate thousands of AI responses automatically
- **Consistent standards** - Remove human bias and subjectivity from evaluation
- **Real-time feedback** - Get immediate quality scores on live production data
- **Focused insights** - Single-purpose evaluators provide actionable feedback
- **Performance tracking** - Monitor model quality trends over time
- **Streamlined workflow** - Reduce manual review burden while maintaining high standards

**Ready to upgrade from spot-checking to comprehensive quality control?**

</details>

## How Evaluation Powers Your Autonomous Engineer

Evaluation data is the foundation that enables your autonomous engineer to detect issues and create fixes:

### **üîç Issue Detection**
- **Quality decline patterns** - Identifies when AI performance drops
- **Error classification** - Categorizes types of failures for targeted fixes
- **Root cause analysis** - Pinpoints which components need improvement

### **üõ†Ô∏è Fix Generation**
- **Success pattern analysis** - Uses high-scoring examples to generate better prompts
- **A/B testing validation** - Compares old vs new prompts using evaluation scores
- **Continuous improvement** - Monitors fix effectiveness over time

### **üìä Autonomous Optimization**
- **Automatic PR creation** - Generates fixes when quality drops below thresholds
- **Evidence-based changes** - Every fix is backed by evaluation data
- **24/7 monitoring** - Never misses quality issues, even during off-hours

## LLM-as-Judge Technology

Leverage powerful language models to assess the quality, accuracy, and relevance of your AI outputs with human-level understanding.

**How it works:**
- **Model integration** - Connect GPT models and Llama (via Together AI) through the platform
- **Automated evaluation** - Run on specified percentage of your production traffic
- **Real-time insights** - Get quality scores within seconds through the dashboard
- **Historical tracking** - Monitor quality trends and improvements over time

![LLM as Judge Concept Diagram](/assets/llm_as_judge/llm_as_judge.png)

## Single-Purpose Evaluator Framework

The key to effective AI evaluation is **focus**. Each evaluator should assess one specific quality dimension for maximum clarity and actionable insights.

<Callout type="warning">
  **Critical Best Practice**: Never combine multiple quality checks in one evaluator. Create separate evaluators for completeness, accuracy, format, empathy, etc. This provides actionable insights and reliable scoring.
</Callout>

### Why Single-Purpose Works Better

**‚ùå What Doesn't Work:**
```
One evaluator checking: helpfulness + accuracy + tone + format + compliance
Result: Vague scores, unclear what needs improvement
```

**‚úÖ What Works:**
```
Completeness Evaluator: Does the response address all parts of the question?
Accuracy Evaluator: Is the information factually correct?
Format Evaluator: Does it follow the required structure?
Empathy Evaluator: Does it show understanding for the user's situation?
```

**Result**: Clear, actionable insights on exactly what to improve

## Platform Workflow

Everything happens through the intuitive Handit.ai platform interface:

### 1. Connect Model Tokens
Add your evaluation models and test connections:
- **GPT-4o models** (OpenAI) 
- **Llama v4 models** (Together AI)
- Test connections and save configurations

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/model_token.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

### 2. Create Focused Evaluators
Build single-purpose evaluators in the Evaluation Suite:
- Define specific evaluation prompts
- Associate with appropriate model tokens
- Focus on one quality dimension per evaluator

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/evaluator_creation.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

### 3. Associate to LLM Nodes
Connect evaluators to your AI functions:
- Link evaluators to specific AI functions
- Set evaluation percentages (5-15% recommended to start)
- Configure priorities for different quality aspects

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/associate_evaluator.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

### 4. Monitor & Analyze
View comprehensive quality insights:
- Real-time evaluation results
- Quality trends across different aspects
- Actionable insights and patterns

![AI Agent Tracing Dashboard](/assets/overview/tracing.png)

![Agent Performance Dashboard](/assets/overview/general-handit.png)

## Quality Dimensions

### Customer Service Evaluators
- **Completeness** - Does the response address all parts of the question?
- **Empathy** - Does it show understanding and care?
- **Solution Clarity** - Are instructions clear and actionable?
- **Policy Compliance** - Does it follow company guidelines?
- **Escalation Appropriateness** - When should it suggest human help?

### Technical Support Evaluators
- **Technical Accuracy** - Is the technical information correct?
- **Safety Check** - Are suggested actions safe?
- **Troubleshooting Flow** - Does it follow logical diagnostic steps?
- **Solution Completeness** - Does it provide a full resolution path?
- **Risk Assessment** - Are there potential negative consequences?

### Content Generation Evaluators
- **Brand Alignment** - Does it match company voice and values?
- **Factual Accuracy** - Is information correct and current?
- **Format Compliance** - Does it follow required structure?
- **Engagement Level** - Is it compelling and interesting?
- **Target Audience** - Is it appropriate for intended readers?

## Business Impact Tracking

Connect evaluation results to real business outcomes:

- **Quality measurement** - Track specific quality dimensions with precision  
- **Business metric correlation** - Link evaluator scores to customer satisfaction and conversions
- **Data-driven insights** - Understand which quality aspects matter most for your business
- **Trend analysis** - Monitor quality patterns over time for each dimension
- **Optimization foundation** - Evaluation data provides the foundation for AI system optimization

## Supported Models

### OpenAI Models
- **GPT-4o** - Highest accuracy for complex evaluations
- **GPT-3.5-turbo** - High-volume evaluation with good performance
- **GPT-4-turbo** - Balanced performance and speed

### Together AI (Llama Models)
- **Llama v4 Scout** - High-quality open source alternative
- **Llama v4 Maverick** - Faster processing for high-volume needs
- **CodeLlama** - Specialized for technical/code evaluation

## Get Started

Ready to give your autonomous engineer the evaluation data it needs to detect issues and create fixes?

<Cards.Card title="Main Quickstart - Includes Evaluation" href="/quickstart" arrow />
<Cards.Card title="Evaluation Deep Dive" href="/evaluation/quickstart" arrow />

**Next Steps:**
- Set up [Autonomous Fixes](/optimization/quickstart) to complete your autonomous engineer
- Explore [Custom Evaluators](/evaluation/guide/custom_evaluators) for specialized quality checks
- Learn about [Evaluation Features](/evaluation/evaluation_features/overview)

<Callout type="success">
  **Transform your AI quality control** from manual spot-checking to autonomous monitoring and fixing with comprehensive evaluation data.
</Callout> 