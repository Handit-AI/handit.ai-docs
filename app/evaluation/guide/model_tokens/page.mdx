---
title: 'Model Token Setup'
sidebarTitle: 'Model Token Setup'
---

import { Callout } from "nextra/components"
import { Steps } from "nextra/components" 

# Model Token Setup

> **Connect AI providers for LLM-as-Judge evaluation.** Model tokens allow Handit.ai to securely access AI providers like OpenAI and Together AI for automated quality assessment of your AI systems.

This guide covers setting up tokens for our supported providers through the platform interface.

<Callout type="info">
  Model tokens are stored securely and encrypted. They're only used for evaluation requests and never shared or logged. All setup happens through the Handit.ai platformâ€”no API integration required.
</Callout>

## Setup Process

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/model_token.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

<Steps>
### Get API Key
Obtain an API key from your AI provider (OpenAI or Together AI)

### Configure Token in Platform
Add the token to Handit.ai through the dashboard interface

### Test Connection
Verify the token works and has appropriate permissions

### Use in Evaluators
Associate the token with your single-purpose evaluators
</Steps>

## Supported Providers

<div className="grid grid-cols-2 gap-4 mt-4">
  <div>
    <h4>ðŸ¤– OpenAI</h4>
    - **GPT-4o** - Highest accuracy for complex evaluations
    - **GPT-3.5-turbo** - Cost-effective for high-volume evaluation
    - **GPT-4-turbo** - Balanced performance and speed
    - Industry standard for evaluation tasks
  </div>
  <div>
    <h4>ðŸ¦™ Together AI</h4>
    - **Llama v4 Scout** - High-quality open source alternative
    - **Llama v4 Maverick** - Faster, cost-effective processing
    - **CodeLlama** - Specialized for technical evaluation
    - Open source model alternatives
  </div>
</div>

## OpenAI Configuration

OpenAI models are the most popular choice for LLM-as-Judge evaluation due to their strong reasoning capabilities.

### Get Your OpenAI API Key

**1. Access OpenAI Platform**
- Visit [OpenAI API Keys](https://platform.openai.com/api-keys)
- Sign in to your OpenAI account
- Click **Create new secret key**

**2. Create and Copy Key**
- Give your key a descriptive name (e.g., "Handit Evaluation")
- Copy the key (starts with `sk-`)
- Store it securelyâ€”you won't see it again

### Add Token to Handit.ai

**1. Navigate to Model Tokens**
- Open your Handit.ai dashboard
- Go to **Settings** â†’ **Model Tokens**
- Click **Add New Token**

**2. Configure Token**

**3. Test and Save**
- Click **Test Connection** to verify functionality
- Save the token configuration once verified

### Model Selection Guide

**GPT-4o** - Best for complex evaluations requiring nuanced reasoning
**GPT-3.5-turbo** - Ideal for high-volume evaluation with good quality
**GPT-4-turbo** - Balanced option with fast response times

## Together AI Configuration

Together AI provides access to open-source models like Llama, offering cost-effective alternatives to proprietary models.

### Get Your Together AI API Key

**1. Access Together AI Platform**
- Visit [Together AI](https://api.together.xyz/)
- Sign up or sign in to your account
- Navigate to **API Keys** section

**2. Create API Key**
- Click **Create new API key**
- Give it a descriptive name
- Copy your API key securely

### Add Token to Handit.ai

**1. Configure Together AI Token**
- In your Handit.ai dashboard, go to **Settings** â†’ **Model Tokens**
- Click **Add New Token**

**2. Test and Save**
- Verify connection with **Test Connection**
- Save the configuration

### Model Selection Guide

**Llama v4 Scout** - High-quality reasoning for complex evaluation tasks
**Llama v4 Maverick** - Fast processing for high-volume evaluation
**CodeLlama** - Specialized for technical content assessment

## Security Best Practices

<div className="grid grid-cols-2 gap-4 mt-4">
  <div>
    <h4>âœ… Token Security</h4>
    - Use dedicated API keys for evaluation only
    - Set usage limits on provider dashboards
    - Rotate keys regularly (monthly/quarterly)
    - Monitor usage through the platform
    - Use descriptive names for easy identification
  </div>
  <div>
    <h4>ðŸ”’ Access Management</h4>
    - Limit team member access to sensitive tokens
    - Use organization/project scoping when available
    - Keep backup tokens for critical evaluations
    - Review token usage regularly
  </div>
</div>

## Common Issues & Solutions

**"Invalid API Key" Error**
- Verify the API key is correct and hasn't expired
- Check if you've reached your usage limits
- Ensure the key has required permissions

**"Rate Limit Exceeded"**
- Check your provider's rate limits
- Consider upgrading your provider plan
- Reduce evaluation frequency temporarily

**"Model Not Found"**
- Verify the model name is exactly correct
- Check if the model is available in your region
- Ensure your API key has access to the selected model

## Using Tokens in Evaluators

Once configured, tokens are used when creating evaluators:

**1. Create Evaluator**
- Go to **Evaluation** â†’ **Evaluation Suite**
- Click **Create New Evaluator**

**2. Select Appropriate Token**
- Choose the token that matches your evaluation complexity
- Consider cost vs. quality trade-offs

**3. Monitor Performance**
- Track token usage through the platform
- Optimize token assignment based on results

## Next Steps

Ready to create your first evaluators?

- [Build single-purpose evaluators](/evaluation/guide/custom_evaluators) using your configured tokens
- [Associate evaluators to LLM nodes](/evaluation/guide/llm_assignment) for automated assessment
- [Monitor evaluation results](/evaluation/guide/evaluation_results) and optimize performance

<Callout type="success">
  Your model tokens are now ready! Next, create single-purpose evaluators that use these tokens to assess specific quality dimensions of your AI's performance.
</Callout> 