---
title: Handit.ai Documentation
description: Handit.ai is an AI-powered platform that helps teams build and deploy intelligent applications at scale.
sidebarTitle: Overview
---

import { Callout } from "nextra/components";
import { Cards } from 'nextra/components'

# Handit.ai

> Handit.ai is an **[open-source optimization platform](https://github.com/Handit-AI/handit.ai)** for AI agents. It offers **end-to-end monitoring**, **automatic evaluation**, and **continuous self-optimization** to deploy the best-performing models and prompts.


<div style={{ 
  background: 'linear-gradient(135deg, #1e293b 0%, #334155 100%)', 
  border: '1px solid #475569',
  borderRadius: '12px', 
  padding: '24px', 
  margin: '24px 0',
  textAlign: 'center'
}}>
  <h3 style={{ margin: '0 0 16px 0', color: '#f8fafc' }}>
    ðŸš€ See Handit.ai in Action - 5 Minute Demo
  </h3>
  <video 
    width="100%" 
    controls
    style={{ 
      borderRadius: '8px', 
      border: '1px solid #64748b',
      maxWidth: '800px',
      margin: '0 auto'
    }}
  >
    <source src="/assets/demo/demo.mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <p style={{ 
    margin: '16px 0 0 0', 
    fontSize: '14px', 
    color: '#cbd5e1',
    fontStyle: 'italic'
  }}>
    Watch how to transform your AI from static to self-improving in minutes
  </p>
</div>

<details>
<summary>**Why Handit.ai?**</summary>

- **Easy to use**: Integrates into your stack in minutes.
- **Auto-optimization**: Deploys top-performing models and prompts instantly.
- **Live monitoring**: Tracks performance and failures in real time.
- **Auto-evaluation**: Grades outputs with LLM-as-Judge and custom KPIs on live data.
- **A/B testing**: Automatically surfaces the best variant by ROI.
- **Impact metrics**: Links AI tweaks to cost savings, conversions, and user satisfaction.
- **Proven**: ASPE.ai saw +62.3% accuracy and +97.8% success rate in 48 hours.

**Get started today and unleash your AI's full potential.**

</details>

<details>
<summary>**AI App Nightmares: What No One Tells You**</summary>

Imagine it's 3 AM and you're staring at your consoleâ€”again. You just fixed that one prompt for the tenth time, only to discover your AI replied to "Uncle Bob" instead of your manager. ðŸ˜±

At first, you laugh it off. It's "just a glitch." Then it happens again: sometimes it ghosts you completely, sometimes it hijacks the CC line, and once it even quoted a message from two weeks ago like a deranged time traveler. You tweak the prompt, pray to the data gods, and whisper "please and thank you" like a mantraâ€¦ but two weeks later, the ghost is back.

This isn't a haunted houseâ€”it's your AI pipeline. And yes, LLMs can gaslight you. They'll whisper "I didn't do it," even as they rewrite your email thread into gibberish. You patch one leak and another springs open. You think, "There has to be a better way."

[Handit.ai](https://github.com/Handit-AI/handit.ai) is the open-source Ghostbuster for your AI nightmares. With live monitoring, automated A/B hunting, and AI-as-Judge evaluations on real traffic, it spots phantom failures and banishes them before they strike again. No more 3 AM panics or unsent apologies.

**Ready to stop chasing ghosts and start shipping rock-solid AI? Dive into Handit.ai**

</details>

<Callout type="info">
  Explore Handit.ai's capabilities. For more details, visit the individual
  documentation pages.
</Callout>

## Real-Time Monitoring

Continuously ingest logs from every model, prompt, and agent in your stack. Instantly visualize performance trends, detect anomalies, and set custom alerts for drift or failuresâ€”live.

![AI Agent Tracing Dashboard](/assets/overview/tracing.png)

<details>
<summary>**Benefits**</summary>

- Ingest logs from models, prompts, and agents in seconds
- Visualize performance trends with interactive dashboards
- Detect anomalies and drift automatically
- Set custom real-time alerts for failures and threshold breaches

</details>

## Evaluation

Run evaluation pipelines on production traffic with custom LLM-as-Judge prompts, business KPI thresholds (accuracy, latency, etc.), and get automated quality scores in real time. Results feed directly into your optimization workflowsâ€”no manual grading required.

![Evaluation Hub Dashboard](/assets/overview/evaluation-hub.png)

![Error Detection and Analysis](/assets/overview/evaluation-error-detection.png)

<details>
<summary>**Benefits**</summary>

- Execute LLM-as-Judge prompts on live traffic
- Enforce business KPI thresholds (accuracy, latency, etc.)
- Receive automated quality scores in real time
- Feed results directly into optimization workflows automatically

</details>

## Self-Optimization & Prompt Management

Spin up experiments on different model versions, prompts, or agent configurations in production. Handit automatically routes traffic, collects performance and ROI metrics, and promotes the winning variantâ€”seamlessly and without human intervention. Manage prompt templates and versions with built-in version control, tag and categorize prompts, and collaborate as a team while tracking performance over time.

![Prompt Performance Comparison](/assets/overview/prompt-comparison.png)

<details>
<summary>**Benefits**</summary>

- Launch experiments across model versions, prompts, or agent configs
- Automatically route traffic and gather performance data
- Compare ROI metrics to identify top performers
- Promote winning variants without manual effort
- Centralize prompt templates and version histories
- Tag, categorize, and collaborate on prompts
- Track prompt performance trends over time
- Roll back or fork proven prompts instantly for quick iteration

</details>

## Get Started

Integrate into your stack in minutes:

<Cards.Card title="Quickstart" href="/quickstart" arrow />

## Get in Touch

Need help? Check out our [GitHub Issues](https://github.com/Handit-AI/handit.ai-docs/issues) or [Contact Us](/contact). 