---
title: Handit.ai Documentation
description: Handit.ai is an AI-powered platform that helps teams build and deploy intelligent applications at scale.
sidebarTitle: Overview
---

import { Callout } from "nextra/components";
import { Cards } from 'nextra/components'

# Handit.ai

> The Open Source Engine that Auto-Improves Your AI. <br />
> Handit evaluates every agent decision, auto-generates better prompts and datasets, A/B-tests the fix, and lets you control what goes live.


<div style={{ 
  background: 'linear-gradient(135deg, #1e293b 0%, #334155 100%)', 
  border: '1px solid #475569',
  borderRadius: '12px', 
  padding: '24px', 
  margin: '24px 0',
  textAlign: 'center'
}}>
  <h3 style={{ margin: '0 0 16px 0', color: '#f8fafc' }}>
    ðŸš€ See Handit.ai in Action - 5 Minute Demo
  </h3>
  <video 
    width="100%" 
    controls
    style={{ 
      borderRadius: '8px', 
      border: '1px solid #64748b',
      maxWidth: '800px',
      margin: '0 auto'
    }}
  >
    <source src="https://storage.googleapis.com/icons-handit/demo%20(1).mp4" type="video/mp4" />
    Your browser does not support the video tag.
  </video>
  <p style={{ 
    margin: '16px 0 0 0', 
    fontSize: '14px', 
    color: '#cbd5e1',
    fontStyle: 'italic'
  }}>
    Watch how to transform your AI from static to self-improving in minutes
  </p>
</div>

<details>
<summary>**Why Handit.ai?**</summary>

- **Easy to use**: Integrates into your stack in minutes.
- **Auto-optimization**: Deploys top-performing models and prompts instantly.
- **Live monitoring**: Tracks performance and failures in real time.
- **Auto-evaluation**: Grades outputs with LLM-as-Judge and custom KPIs on live data.
- **A/B testing**: Automatically surfaces the best variant by ROI.
- **Impact metrics**: Links AI tweaks to cost savings, conversions, and user satisfaction.
- **Proven**: ASPE.ai saw +62.3% accuracy and +97.8% success rate in 48 hours.

**Get started today and unleash your AI's full potential with [Handit.ai](https://beta.handit.ai/).**

</details>

<details>
<summary>**AI App Nightmares: What No One Tells You**</summary>

Imagine it's 3 AM and you're staring at your consoleâ€”again. You just fixed that one prompt for the tenth time, only to discover your AI replied to "Uncle Bob" instead of your manager. ðŸ˜±

At first, you laugh it off. It's "just a glitch." Then it happens again: sometimes it ghosts you completely, sometimes it hijacks the CC line, and once it even quoted a message from two weeks ago like a deranged time traveler. You tweak the prompt, pray to the data gods, and whisper "please and thank you" like a mantraâ€¦ but two weeks later, the ghost is back.

This isn't a haunted houseâ€”it's your AI pipeline. And yes, LLMs can gaslight you. They'll whisper "I didn't do it," even as they rewrite your email thread into gibberish. You patch one leak and another springs open. You think, "There has to be a better way."

[Handit.ai](https://github.com/Handit-AI/handit.ai) is the open-source Ghostbuster for your AI nightmares. With live monitoring, automated A/B hunting, and AI-as-Judge evaluations on real traffic, it spots phantom failures and banishes them before they strike again. No more 3 AM panics or unsent apologies.

**Ready to stop chasing ghosts and start shipping rock-solid AI? Dive into [Handit.ai](https://beta.handit.ai/)**

</details>

<Callout type="info">
  Explore Handit.ai's capabilities. For more details, visit the individual
  documentation pages.
</Callout>

## Real-Time Monitoring

Continuously ingest logs from every model, prompt, and agent in your stack. Instantly visualize performance trends, detect anomalies, and set custom alerts for drift or failuresâ€”live.

Ready to evaluate your AI performance? Visit [Evaluation Hub](https://beta.handit.ai/ag-tracing)

![AI Agent Tracing Dashboard](/assets/overview/tracing.png)

<details>
<summary>**Benefits**</summary>

- Ingest logs from models, prompts, and agents in seconds
- Visualize performance trends with interactive dashboards
- Detect anomalies and drift automatically
- Set custom real-time alerts for failures and threshold breaches

</details>

## Evaluation

Run evaluation pipelines on production traffic with custom LLM-as-Judge prompts, business KPI thresholds (accuracy, latency, etc.), and get automated quality scores in real time. Results feed directly into your optimization workflowsâ€”no manual grading required.

Run your evaluations here: [Evaluation Hub](https://beta.handit.ai/evaluation-hub)

![Evaluation Hub Dashboard](/assets/overview/evaluation-hub.png)

![Error Detection and Analysis](/assets/overview/evaluation-error-detection.png)

<details>
<summary>**Benefits**</summary>

- Execute LLM-as-Judge prompts on live traffic
- Enforce business KPI thresholds (accuracy, latency, etc.)
- Receive automated quality scores in real time
- Feed results directly into optimization workflows automatically

</details>

## Prompt Management, Self-Optimization, and AI CI/CD

- **Run experiments**  
  Test different model versions, prompts, or agent configurations with A/B traffic routingâ€”no manual work required.

- **Automatically optimize**  
  Handit collects performance and ROI metrics in real time, then promotes the winning variant without human intervention.

- **Get the best prompt from Handit**  
  Compare prompt versions side-by-side, promote your favorite to production, and deploy it with a single click.

- **Collaborate and track**  
  Use built-in version control to manage templates, tag and categorize prompts, and view performance trends over time.


Run your prompt experiments and deployments here: [Prompt Versions](https://beta.handit.ai/prompt-versions)

![Prompt Performance Comparison](/assets/overview/prompt-comparison.png)

<details>
<summary>**Benefits**</summary>

- Launch experiments across model versions, prompts, or agent configs
- Automatically route traffic and gather performance data
- Compare ROI metrics to identify top performers
- Promote winning variants without manual effort
- Centralize prompt templates and version histories
- Tag, categorize, and collaborate on prompts
- Track prompt performance trends over time
- Roll back or fork proven prompts instantly for quick iteration

</details>

## Get Started

Integrate into your stack in minutes:

<Cards.Card title="Quickstart" href="/quickstart" arrow />

## Get in Touch

Need help? Check out our [GitHub Issues](https://github.com/Handit-AI/handit.ai-docs/issues) or [Contact Us](/more/contact). 