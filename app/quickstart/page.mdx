---
title: Quickstart
description: Get started with Handit.ai's complete AI observability and optimization platform in under 30 minutes.
sidebarTitle: Quickstart
---

import { Callout } from "nextra/components";
import { Steps } from "nextra/components";
import { Tabs } from "nextra/components";

# Complete Handit.ai Quickstart

> **The autonomous engineer that fixes your AI 24/7** <br />
> Handit catches failures, writes fixes, tests them, and ships PRs, automatically. Like having an on-call engineer dedicated to your AI, except it works 24/7.

<Callout type="info">
  **What you'll build:** A fully observable, continuously evaluated, and automatically optimizing AI system that improves itself based on real production data.
</Callout>

## Overview: The Complete Journey

Here's what we'll accomplish in three phases:

<Steps>
### [Phase 1: AI Observability](#phase-1-ai-observability-3-minutes) ‚è±Ô∏è 3 minutes
Set up comprehensive tracing to see inside your AI agents and understand what they're doing

### [Phase 2: Quality Evaluation](#phase-2-quality-evaluation-5-minutes) ‚è±Ô∏è 5 minutes
Add automated evaluation to continuously assess performance across multiple quality dimensions

### [Phase 3: Autonomous AI Fixes](#phase-3-autonomous-ai-fixes-8-minutes) ‚è±Ô∏è 2 minutes
Connect GitHub so Handit can automatically fix issues by creating pull requests with improved system prompts
</Steps>

<Callout type="success">
  **The Result**: Complete visibility into performance with automated optimization recommendations based on real production data.
</Callout>

## Prerequisites

Before we start, make sure you have:

- A [Handit.ai Account](https://beta.handit.ai) (sign up if needed)
- Node.js installed (for the CLI)
- 15-20 minutes to complete the setup

## Phase 1: AI Observability (3 minutes)

Let's add comprehensive tracing to see exactly what your AI is doing.

### Step 1: Install the Handit CLI

```bash filename="terminal"
npm install -g @handit.ai/cli
```

### Step 2: Set Up Your Project

Run the setup command and follow the interactive prompts:

```bash filename="terminal"
handit-cli setup
```

The CLI will:
- Guide you through connecting your Handit.ai account
- Generate all the required integration code for your project
- Set up tracing for your AI agents automatically
- Create the necessary configuration files

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px'}}
>
  <source src="/assets/quickstart/cli-setup-app-2.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

### Step 3: Start Your Application

The CLI has generated all the code you need. Simply run your application and start seeing traces in your dashboard immediately.

<Callout type="success">
  **That's it!** Your AI agent now has full observability with automatic tracking of all operations, timing, inputs, outputs, and errors.
</Callout>

<Callout type="success">
  **Phase 1 Complete!** üéâ You now have full observability with every operation, timing, input, output, and error visible in your dashboard.
</Callout>

**‚û°Ô∏è Want to dive deeper?** Check out our [detailed Tracing Quickstart](/tracing/quickstart) for advanced features and best practices.

## Phase 2: Quality Evaluation (5 minutes)

Now let's add automated evaluation to continuously assess quality across multiple dimensions.

### Step 1: Set Up Evaluators with CLI

Run the evaluators setup command:

```bash filename="terminal"
handit-cli evaluators-setup
```

The CLI will guide you through:
- Connecting your evaluation models (OpenAI, Together AI, etc.)
- Adding AI model tokens for evaluation
- Connecting existing evaluators to your LLM nodes
- Setting up evaluation percentages and priorities

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px' }}
>
  <source src="/assets/quickstart/evaluators-setup.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

### Step 2: Monitor Results

View real-time evaluation results in:
- **Tracing** tab: Individual evaluation scores
- **Agent Performance**: Quality trends over time

**Tracing Dashboard - Individual Evaluation Scores:**
![AI Agent Tracing Dashboard](/assets/overview/tracing.png)


**‚û°Ô∏è Want more sophisticated evaluators?** Check out our [detailed Evaluation Quickstart](/evaluation/quickstart) for advanced techniques.

## Phase 3: Autonomous AI Fixes (2 minutes)

Connect GitHub so Handit can automatically fix issues by creating pull requests with improved system prompts.

### Step 1: Connect Your Repository

Run the GitHub integration command:

```bash filename="terminal"
handit-cli github
```

The CLI will guide you through:
- Installing the Handit GitHub app on your repository
- Configuring repository permissions
- Setting up automated pull request creation for fixes

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px' }}
>
  <source src="/assets/quickstart/github-setup.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

<Callout type="tip">
  **How it works:** Once connected, Handit continuously monitors your AI's performance. When it detects issues and creates fixes, it automatically opens pull requests in your repository with improved system prompts. You simply review and merge the fixes you want.
</Callout>

### Step 2: Automatic Issue Detection & Fixes

Here's what happens automatically after GitHub integration:

**1. Continuous Monitoring**
- Handit analyzes every AI interaction for quality issues
- Detects patterns in failures, low scores, or user complaints

**2. Automatic Fix Generation**  
- When issues are detected, Handit generates improved system prompts
- Tests the fixes against real data to ensure they work
- Validates improvements with statistical confidence

**3. Pull Request Creation**
- Handit creates a PR in your repository
- Replaces the problematic system prompt in your code
- Includes performance data showing the improvement

**4. Review & Deploy**
- You review the PR like any other code change
- Merge to deploy the fix automatically
- Reject if you don't want the change

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px' }}
>
  <source src="/assets/quickstart/pr-review.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

**Example:** If your customer service AI starts giving incomplete responses, Handit will detect this pattern, generate a better system prompt that addresses completeness, test it, and create a PR to replace the old prompt in your codebase.

<Callout type="success">
  **Phase 3 Complete!** üéâ You now have an autonomous AI engineer that detects issues, generates fixes, and creates pull requests automatically. Your AI will continuously improve itself through your normal GitHub workflow.
</Callout>

**‚û°Ô∏è Want advanced autonomous fixing features?** Check out our [detailed Autonomous AI Fixes guide](/optimization/quickstart) for advanced GitHub integration and deployment strategies.

## What You've Accomplished

Congratulations! You now have a complete AI observability and optimization system set up in under 20 minutes:

### ‚úÖ Full Observability
- Complete visibility into operations with CLI-generated tracing
- Real-time monitoring of all LLM calls and tools
- Detailed execution traces with timing and error tracking

### ‚úÖ Continuous Evaluation  
- Automated quality assessment configured via CLI
- Real-time evaluation scores and trends
- Quality insights to identify improvement opportunities

### ‚úÖ Autonomous AI Fixes with GitHub Integration
- Automatic detection of quality issues and performance problems
- AI-generated system prompt fixes tested against real data
- Automated pull requests that replace problematic prompts in your codebase
- Review and deploy fixes through your normal GitHub workflow

## Next Steps

- Join our [Discord community](https://discord.gg/wZbW9Bu5) for support
- Check out [GitHub Issues](https://github.com/Handit-AI/handit.ai-docs/issues) for additional help
- Explore [Tracing](/tracing/overview) to monitor your AI agents
- Set up [Evaluation](/evaluation/overview) to grade your AI outputs
- Configure [Autonomous AI Fixes](/optimization/overview) for continuous improvement

## Resources

- [Tracing Documentation](/tracing/overview) - Monitor AI agent performance
- [Evaluation Documentation](/evaluation/overview) - Grade AI outputs automatically  
- [Autonomous AI Fixes Documentation](/optimization/overview) - Your autonomous engineer that works 24/7
- Visit our [GitHub Issues](https://github.com/Handit-AI/handit.ai-docs/issues) page

<Callout type="info">
  **Ready to transform your AI?** Visit [beta.handit.ai](https://beta.handit.ai) to get started with the complete Handit.ai platform today.
</Callout>

## Troubleshooting

**CLI Setup Issues?**
- Ensure Node.js is installed: `node --version`
- Try reinstalling the CLI: `npm uninstall -g @handit.ai/cli && npm install -g @handit.ai/cli`
- Check your Handit.ai account credentials

**Tracing Not Working?**
- Run `handit-cli setup` again to regenerate configuration
- Verify your generated code is being executed
- Check that your API key was set correctly during setup

**Evaluations Not Running?**
- Re-run `handit-cli evaluators-setup` to verify connections
- Confirm model tokens are valid and have sufficient credits
- Verify LLM nodes are receiving traffic

**GitHub Integration Issues?**
- Ensure you have admin access to the repository
- Try running `handit-cli github` again to reinstall the app
- Check repository permissions in your GitHub settings

**Need Help?**
- Visit our [Support](/more/contact) page
- Join our [Discord community](https://discord.gg/wZbW9Bu5)
- Check individual quickstart guides for detailed troubleshooting
