---
title: Quickstart
description: Get started with Handit.ai's complete AI observability and optimization platform in under 30 minutes.
sidebarTitle: Quickstart
---

import { Callout } from "nextra/components";
import { Steps } from "nextra/components";
import { Tabs } from "nextra/components";

# Complete Handit.ai Quickstart

> **Transform your AI from black box to self-improving system in under 30 minutes.** This guide walks you through the complete Handit.ai journey: observability, evaluation, and automated optimization.

<Callout type="info">
  **What you'll build:** A fully observable, continuously evaluated, and automatically optimizing AI system that improves itself based on real production data.
</Callout>

## Overview: The Complete Journey

Here's what we'll accomplish in three phases:

<Steps>
### [Phase 1: AI Observability](#phase-1-ai-observability-5-minutes) ⏱️ 5 minutes
Set up comprehensive tracing to see inside your AI agents and understand what they're doing

### [Phase 2: Quality Evaluation](#phase-2-quality-evaluation-10-minutes) ⏱️ 10 minutes
Add automated evaluation to continuously assess performance across multiple quality dimensions

### [Phase 3: Self-Improving AI](#phase-3-self-improving-ai-15-minutes) ⏱️ 15 minutes
Enable automatic optimization that generates better prompts, tests them, and provides proven improvements
</Steps>

<Callout type="success">
  **The Result**: Complete visibility into performance with automated optimization recommendations based on real production data.
</Callout>

## Prerequisites

Before we start, make sure you have:

- A [Handit.ai Account](https://beta.handit.ai) (sign up if needed)
- 15-30 minutes to complete the setup

## Phase 1: AI Observability (5 minutes)

Let's add comprehensive tracing to see exactly what your AI is doing.

### Step 1: Install the SDK

<Tabs items={["Python", "JavaScript"]} defaultIndex="0">
<Tabs.Tab>
```bash filename="terminal"
pip install -U "handit-sdk>=1.9.0"
```
</Tabs.Tab>
<Tabs.Tab>
```bash filename="terminal"
npm install @handit.ai/node
```
</Tabs.Tab>
</Tabs>

### Step 2: Get Your Integration Token

1. Log into your [Handit.ai Dashboard](https://beta.handit.ai)
2. Go to **Settings** → **Integrations**
3. Copy your integration token

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/integration_token.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

### Step 3: Create Your Agent Structure

1. Navigate to **"My Agents"** in the dashboard
2. Click **"Create New Agent"**
3. Define your agent's workflow using either:
   - **AI-Powered Creation**: Upload a JSON description of your agent
   - **Visual Builder**: Drag and drop to build your workflow
4. Download your `handit.config.json` file

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/create_agent.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

5. Example of handit.config.json, this is an example, yours will be different:

```json filename="handit.config.json"
{
    "nodes": [
        {"slug": "customer-service-interaction", "type": "agent"},
        {"slug": "knowledge-base-search", "type": "tool"},
        {"slug": "response-generator", "type": "llm"}
    ]
}
```

### Step 4: Add Basic Tracing

Now, let's set up your main agent function, LLM calls and tool usage with tracing. You'll need to set up four key components:
1. Initialize Handit.ai service
2. Set up main orchestration with `startAgentTracing()`
3. Track LLM calls with `captureModel()`
4. Track tool usage with `traceAgentNode()`

Make sure you have your `handit.config.json` ready with the correct slugs for each component [See Step 3](#step-3-create-your-agent-structure). These slugs will be used to identify and track each part of your agent's workflow.

<Tabs items={["Python", "JavaScript"]} defaultIndex="0">
<Tabs.Tab>

Create a `handit_service.py` file to initialize the Handit.ai tracker

```python filename="handit_service.py"
"""
Handit.ai service initialization.
This file creates a singleton tracker instance that can be imported across your application.
"""
from handit import HanditTracker

# Create a singleton tracker instance
tracker = HanditTracker()
# Configure with your API key from environment variables
tracker.config(api_key=os.getenv("HANDIT_API_KEY"))
```
</Tabs.Tab>
<Tabs.Tab>

Create a `handit_service.js` file to initialize the Handit.ai tracker

```javascript filename="handit_service.js"
/**
 * Handit.ai service initialization.
 * This file creates a singleton configuration that can be imported across your application.
 */
import { config } from '@handit.ai/node';

// Configure with your API key from environment variables
config({ apiKey: process.env.HANDIT_API_KEY });
```
</Tabs.Tab>
</Tabs>

Now, set up your main agent function, LLM calls and tool usage. By this part you will need your slugs, `handit.config.json` [See Step 3](#step-3-create-your-agent-structure).

<Tabs items={["Python", "JavaScript"]} defaultIndex="0">
<Tabs.Tab>
Functions used for this example: 

- `@tracker.start_agent_tracing()`
   - Type: Decorator
   - Usage: Applied to main orchestration function
   - Parameters: None
   - Purpose: Marks start of agent tracing for entire request flow

- `tracker._send_tracked_data()`
   - Type: Async function
   - Parameters:
     - `model_id`: str (required) - Must match slug from handit.config.json
     - `request_body`: dict (required) - Input data for operation
     - `response_body`: dict (required) - Output data from operation
   - Purpose: Tracks individual component executions (LLMs/Tools)

```python filename="customer_service_agent.py"
"""
Customer Service Agent with comprehensive tracing.

This example demonstrates the correct pattern for implementing tracing in a customer service agent:
1. Use @tracker.start_agent_tracing() ONLY on the main orchestration function
2. Use tracker._send_tracked_data() for tracking individual component executions
3. Each component should track its own execution with appropriate model_id

The main orchestration function (handle_customer_request) is the only entry point that should
use start_agent_tracing. All other components should use _send_tracked_data directly.

Example handit.config.json slugs used in this code:
{
    "nodes": [
        {"slug": "customer-service-interaction", "type": "agent"},
        {"slug": "knowledge-base-search", "type": "tool"},
        {"slug": "response-generator", "type": "llm"}
    ]
}
"""
from handit_service import tracker
import time
from typing import Dict, Any

async def search_knowledge_base(query: str) -> Dict[str, Any]:
    """
    Search the knowledge base for relevant information.
    
    This function demonstrates how to track a tool execution:
    1. Execute the tool logic
    2. Track the execution using _send_tracked_data
    3. Return the results
    
    Args:
        query (str): The search query
        
    Returns:
        Dict[str, Any]: Search results with confidence score
    """
    # Simulate knowledge base search
    await asyncio.sleep(0.5)  # Simulate API call
    results = {
        "results": [
            {"title": "Refund Policy", "content": "Customers can request refunds within 30 days..."},
            {"title": "Shipping Info", "content": "Standard shipping takes 3-5 business days..."}
        ],
        "confidence": 0.85
    }
    
    # Track the knowledge base search
    await tracker._send_tracked_data(
        model_id="knowledge-base-search",  # Must match slug in handit.config.json
        request_body={"query": query},
        response_body=results
    )
    
    return results

async def generate_response(context: Dict[str, Any]) -> str:
    """
    Generate a response using LLM.
    
    This function demonstrates how to track an LLM call:
    1. Execute the LLM call
    2. Track the execution using _send_tracked_data
    3. Return the response
    
    Args:
        context (Dict[str, Any]): Context including user message and KB results
        
    Returns:
        str: Generated response
    """
    # Simulate LLM call
    await asyncio.sleep(1)  # Simulate API call
    response = "Based on our policies, you can request a refund within 30 days of purchase..."
    
    # Track the LLM response generation
    await tracker._send_tracked_data(
        model_id="response-generator",  # Must match slug in handit.config.json
        request_body=context,
        response_body={"response": response}
    )
    
    return response

# Main customer service agent - this is the only function that uses start_agent_tracing
@tracker.start_agent_tracing()
async def handle_customer_request(user_message: str, user_id: str) -> Dict[str, Any]:
    """
    Process a customer service request with comprehensive tracing.
    
    This is the main orchestration function that:
    1. Uses @tracker.start_agent_tracing() to track the entire request flow
    2. Coordinates between different components (KB search, LLM)
    3. Tracks the complete interaction
    
    The start_agent_tracing decorator should ONLY be used on this main function,
    as it's the entry point that orchestrates the entire flow.
    
    Args:
        user_message (str): The customer's message
        user_id (str): The customer's ID
        
    Returns:
        Dict[str, Any]: Response with answer and metadata
    """
    # Track the start of processing
    start_time = time.time()
    
    # Search knowledge base
    kb_results = await search_knowledge_base(user_message)
    
    # Generate response using LLM
    response = await generate_response({
        "user_message": user_message,
        "kb_results": kb_results
    })
    
    return {
        "response": response,
        "processing_time": time.time() - start_time,
        "confidence": kb_results["confidence"]
    }

# Usage example
async def main():
    """
    Example usage of the customer service agent.
    
    This demonstrates how to:
    1. Call the main orchestration function
    2. Handle the response
    3. Process any errors
    """
    try:
        result = await handle_customer_request(
            "I want to return my order, it's been 20 days",
            "user_123"
        )
        print(result)
    except Exception as e:
        print(f"Error processing request: {e}")
```
</Tabs.Tab>
<Tabs.Tab>
Functions used for this example: 

- `startAgentTracing()`
  - Type: Function wrapper
  - Usage: Wraps main orchestration function
  - Parameters: None
  - Purpose: Marks start of agent tracing for entire request flow

- `captureAgentNode()`
  - Type: Async function
  - Parameters:
    - `agentNodeSlug`: string (required) - Must match slug from handit.config.json
    - `requestBody`: object (required) - Input data for operation
    - `responseBody`: object (required) - Output data from operation
  - Purpose: Tracks individual component executions (LLMs/Tools)

```javascript filename="customer_service_agent.js"
/**
 * Customer Service Agent with comprehensive tracing.
 * 
 * This example demonstrates the correct pattern for implementing tracing in a customer service agent:
 * 1. Use startAgentTracing() ONLY on the main orchestration function
 * 2. Use captureAgentNode() for tracking individual component executions
 * 3. Each component should track its own execution with appropriate agentNodeSlug
 * 
 * The main orchestration function (handleCustomerRequest) is the only entry point that should
 * use startAgentTracing. All other components should use captureAgentNode directly.
 * 
 * Example handit.config.json slugs used in this code:
 * {
 *     "nodes": [
 *         {"slug": "customer-service-interaction", "type": "agent"},
 *         {"slug": "knowledge-base-search", "type": "tool"},
 *         {"slug": "response-generator", "type": "llm"}
 *     ]
 * }
 */
import { startAgentTracing, captureAgentNode } from '@handit.ai/node';

/**
 * Search the knowledge base for relevant information.
 * 
 * This function demonstrates how to track a tool execution:
 * 1. Execute the tool logic
 * 2. Track the execution using captureAgentNode
 * 3. Return the results
 * 
 * @param {string} query - The search query
 * @returns {Promise<Object>} Search results with confidence score
 */
async function searchKnowledgeBase(query) {
    // Simulate knowledge base search
    await new Promise(resolve => setTimeout(resolve, 500)); // Simulate API call
    const results = {
        results: [
            { title: 'Refund Policy', content: 'Customers can request refunds within 30 days...' },
            { title: 'Shipping Info', content: 'Standard shipping takes 3-5 business days...' }
        ],
        confidence: 0.85
    };
    
    // Track the knowledge base search
    await captureAgentNode({
        agentNodeSlug: 'knowledge-base-search',  // Must match slug in handit.config.json
        requestBody: { query },
        responseBody: results
    });
    
    return results;
}

/**
 * Generate a response using LLM.
 * 
 * This function demonstrates how to track an LLM call:
 * 1. Execute the LLM call
 * 2. Track the execution using captureAgentNode
 * 3. Return the response
 * 
 * @param {Object} context - Context including user message and KB results
 * @returns {Promise<string>} Generated response
 */
async function generateResponse(context) {
    // Simulate LLM call
    await new Promise(resolve => setTimeout(resolve, 1000)); // Simulate API call
    const response = 'Based on our policies, you can request a refund within 30 days of purchase...';
    
    // Track the LLM response generation
    await captureAgentNode({
        agentNodeSlug: 'response-generator',  // Must match slug in handit.config.json
        requestBody: context,
        responseBody: { response }
    });
    
    return response;
}

/**
 * Process a customer service request with comprehensive tracing.
 * 
 * This is the main orchestration function that:
 * 1. Uses startAgentTracing() to track the entire request flow
 * 2. Coordinates between different components (KB search, LLM)
 * 3. Tracks the complete interaction
 * 
 * The startAgentTracing wrapper should ONLY be used on this main function,
 * as it's the entry point that orchestrates the entire flow.
 * 
 * @param {string} userMessage - The customer's message
 * @param {string} userId - The customer's ID
 * @returns {Promise<Object>} Response with answer and metadata
 */
const handleCustomerRequest = startAgentTracing(async (userMessage, userId) => {
    const startTime = Date.now();
    
    // Search knowledge base
    const kbResults = await searchKnowledgeBase(userMessage);
    
    // Generate response using LLM
    const response = await generateResponse({
        userMessage,
        kbResults
    });
    
    return {
        response,
        processingTime: Date.now() - startTime,
        confidence: kbResults.confidence
    };
});

/**
 * Example usage of the customer service agent.
 * 
 * This demonstrates how to:
 * 1. Call the main orchestration function
 * 2. Handle the response
 * 3. Process any errors
 */
async function main() {
    try {
        const result = await handleCustomerRequest(
            'I want to return my order, it\'s been 20 days',
            'user_123'
        );
        console.log(result);
    } catch (error) {
        console.error('Error processing request:', error);
    }
}
```
</Tabs.Tab>
</Tabs>

<Callout type="success">
  **Phase 1 Complete!** 🎉 You now have full observability with every operation, timing, input, output, and error visible in your dashboard.
</Callout>

**➡️ Want to dive deeper?** Check out our [detailed Tracing Quickstart](/tracing/quickstart) for advanced features and best practices.

## Phase 2: Quality Evaluation (10 minutes)

Now let's add automated evaluation to continuously assess quality across multiple dimensions.

### Step 1: Connect Evaluation Models

1. Go to **Settings** → **Model Tokens**
2. Add your OpenAI or other model credentials
3. These models will act as "judges" to evaluate responses


<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/model_token.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>


### Step 2: Create Focused Evaluators

Create separate evaluators for each quality aspect. **Critical principle**: One evaluator = one quality dimension.

1. Go to **Evaluation** → **Evaluation Suite**
2. Click **Create New Evaluator**

**Example Evaluator 1: Response Completeness**
```
You are evaluating whether an AI response completely addresses the user's question.

Focus ONLY on completeness - ignore other quality aspects.

User Question: {input}
AI Response: {output}

Rate on a scale of 1-10:
1-2 = Missing major parts of the question
3-4 = Addresses some parts but incomplete
5-6 = Addresses most parts adequately  
7-8 = Addresses all parts well
9-10 = Thoroughly addresses every aspect

Output format:
Score: [1-10]
Reasoning: [Brief explanation]
```

**Example Evaluator 2: Accuracy Check**
```
You are checking if an AI response contains accurate information.

Focus ONLY on factual accuracy - ignore other aspects.

User Question: {input}
AI Response: {output}

Rate on a scale of 1-10:
1-2 = Contains obvious false information
3-4 = Contains questionable claims
5-6 = Mostly accurate with minor concerns
7-8 = Accurate information
9-10 = Completely accurate and verifiable

Output format:
Score: [1-10]
Reasoning: [Brief explanation]
```

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/evaluator_creation.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

### Step 3: Associate Evaluators to Your LLM Nodes

1. Go to **Agent Performance**
2. Select your LLM node (e.g., "response-generator")
3. Click on Manage Evaluators on the menu
4. Add your evaluators

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/associate_evaluator.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>
### Step 4: Monitor Results

View real-time evaluation results in:
- **Tracing** tab: Individual evaluation scores
- **Agent Performance**: Quality trends over time

**Tracing Dashboard - Individual Evaluation Scores:**
![AI Agent Tracing Dashboard](/assets/overview/tracing.png)

**Agent Performance Dashboard - Quality Trends:**
![Agent Performance Dashboard](/assets/overview/general-handit.png)

<Callout type="success">
  **Phase 2 Complete!** 🎉 Continuous evaluation is now running across multiple quality dimensions with real-time insights into performance trends.
</Callout>

**➡️ Want more sophisticated evaluators?** Check out our [detailed Evaluation Quickstart](/evaluation/quickstart) for advanced techniques.

## Phase 3: Self-Improving AI (15 minutes)

Finally, let's enable automatic optimization that generates better prompts and provides proven improvements.

### Step 1: Connect Optimization Models

1. Go to **Settings** → **Model Tokens**
2. Select optimization model tokens
3. Self-improving AI automatically activates once configured

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/model_token.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

<Callout type="tip">
  **Automatic Activation**: Once optimization tokens are configured, the system automatically begins analyzing evaluation data and generating optimizations. No additional setup required!
</Callout>

### Step 2: Monitor Optimization Results

The system is now automatically generating and testing improved prompts. Monitor results in two places:

**Agent Performance Dashboard:**
- View agent performance metrics
- Compare current vs optimized versions
- See improvement percentages

![Agent Performance Dashboard](/assets/overview/general-handit.png)

**Release Hub:**
- Go to **Optimization** → **Release Hub**
- View detailed prompt comparisons
- See statistical confidence and recommendations

![Release Hub - Prompt Performance Comparison](/assets/overview/prompt-comparison.png)

### Step 3: Deploy Optimizations

1. **Review Recommendations** in Release Hub
2. **Compare Performance** between current and optimized prompts
3. **Mark as Production** for prompts you want to deploy
4. **Fetch via SDK** in your application

<video 
  width="100%" 
  autoPlay 
  loop 
  muted 
  playsInline
  style={{ borderRadius: '8px', border: '1px solid #e5e7eb' }}
>
  <source src="/assets/quickstart/ci:cd.mp4" type="video/mp4" />
  Your browser does not support the video tag.
</video>

**Fetch Optimized Prompts:**

<Tabs items={["Python", "JavaScript"]} defaultIndex="0">
<Tabs.Tab>
```python filename="optimization_integration.py"
from handit import HanditTracker

# Initialize tracker
tracker = HanditTracker(api_key="your-api-key")

# Fetch current production prompt
optimized_prompt = tracker.fetch_optimized_prompt(
    model_id="response-generator"
)

# Use in your LLM calls
response = your_llm_client.chat.completions.create(
    model="gpt-4",
    messages=[
        {"role": "system", "content": optimized_prompt},
        {"role": "user", "content": user_query}
    ]
)
```
</Tabs.Tab>
<Tabs.Tab>
```javascript filename="optimization_integration.js"
import { HanditClient } from '@handit/sdk';

const handit = new HanditClient({ apiKey: 'your-api-key' });

// Fetch current production prompt
const optimizedPrompt = await handit.fetchOptimizedPrompt({ 
  modelId: 'response-generator' 
});

// Use in your LLM calls
const response = await openai.chat.completions.create({
  model: 'gpt-4',
  messages: [
    { role: 'system', content: optimizedPrompt },
    { role: 'user', content: userQuery }
  ]
});
```
</Tabs.Tab>
</Tabs>

<Callout type="success">
  **Phase 3 Complete!** 🎉 You now have a self-improving AI that automatically detects quality issues, generates better prompts, tests them in the background, and provides proven improvements.
</Callout>

**➡️ Want advanced optimization features?** Check out our [detailed Optimization Quickstart](/optimization/quickstart) for CI/CD integration and deployment strategies.

## What You've Accomplished

Congratulations! You now have a complete AI observability and optimization system:

### ✅ Full Observability
- Complete visibility into operations
- Real-time monitoring of all LLM calls and tools
- Detailed execution traces with timing and error tracking

### ✅ Continuous Evaluation  
- Automated quality assessment across multiple dimensions
- Real-time evaluation scores and trends
- Quality insights to identify improvement opportunities

### ✅ Self-Improving AI
- Automatic detection of quality issues
- AI-generated prompt optimizations
- Background A/B testing with statistical confidence
- Production-ready improvements delivered via SDK

## Next Steps

- Join our [Discord community](https://discord.gg/M6su47HZ) for support
- Check out [GitHub Issues](https://github.com/Handit-AI/handit.ai-docs/issues) for additional help
- Explore [Tracing](/tracing) to monitor your AI agents
- Set up [Evaluation](/evaluation) to grade your AI outputs
- Configure [Optimization](/optimization) for continuous improvement

## Resources

- [Tracing Documentation](/tracing) - Monitor AI agent performance
- [Evaluation Documentation](/evaluation) - Grade AI outputs automatically  
- [Optimization Documentation](/optimization) - Improve prompts continuously
- Visit our [GitHub Issues](https://github.com/Handit-AI/handit.ai-docs/issues) page

<Callout type="info">
  **Ready to transform your AI?** Visit [beta.handit.ai](https://beta.handit.ai) to get started with the complete Handit.ai platform today.
</Callout>

## Troubleshooting

**Tracing Not Working?**
- Verify your API key is correct and set as environment variable
- Check that your agent structure matches your `handit.config.json`
- Ensure you're using the correct node slugs in your tracing code

**Evaluations Not Running?**
- Confirm model tokens are valid and have sufficient credits
- Verify LLM nodes are receiving traffic
- Check evaluation percentages are > 0%

**Optimizations Not Generating?**
- Ensure evaluation data shows quality issues (scores below threshold)
- Verify optimization model tokens are configured
- Confirm sufficient evaluation data has been collected

**Need Help?**
- Visit our [Support](/more/contact) page
- Join our [Discord community](https://discord.gg/M6su47HZ)
- Check individual quickstart guides for detailed troubleshooting
