---
title: 'Tracing Features Overview'
sidebarTitle: 'Overview'
---

import { Callout } from "nextra/components"
import { Steps } from "nextra/components"
import { Tabs } from "nextra/components"
import { Cards } from 'nextra/components'

# Tracing Features Overview

> **Transform your AI agents from black boxes to fully observable systems.** Handit.ai's tracing features give you complete visibility into every operation, from the highest-level agent workflow to individual LLM calls and tool executions.

Handit.ai provides three core tracing capabilities that work together to give you comprehensive observability:

- **ü§ñ Agent Tracing** - The orchestrator that tracks your entire agent workflow from start to finish
- **üß† LLM Node Tracing** - The brain monitor that captures every AI model interaction with complete context  
- **üõ†Ô∏è Tool Tracing** - The action tracker that monitors every function and tool execution

## The Three Pillars of AI Agent Tracing

### ü§ñ Agent Tracing: The Complete Picture

**What it does:** Wraps your entire agent workflow, creating the root trace that connects all operations together.

**Perfect for:** Main agent functions, API endpoints, end-to-end monitoring, production workflows

**Key capabilities:**
- Automatic operation linking - All child operations are automatically connected
- Complete execution timeline - From first input to final output
- Error propagation tracking - See how errors flow through your system
- Performance overview - Total execution time and resource usage

<Tabs items={['Python', 'JavaScript']}>
<Tabs.Tab>
```python filename="process_customer_request.py"
@tracker.start_agent_tracing()
async def process_customer_request(request):
    # Everything inside here is automatically traced
    intent = await classify_intent(request)
    context = await search_knowledge_base(intent)
    response = await generate_response(context)
    return response
```
</Tabs.Tab>
<Tabs.Tab>
```javascript filename="processCustomerRequest.js"
const processCustomerRequest = startAgentTracing(async (request) => {Add commentMore actions
    // Everything inside here is automatically traced
    const intent = await classifyIntent(request);
    const context = await searchKnowledgeBase(intent);
    const response = await generateResponse(context);
    return response;
});
```
</Tabs.Tab>
</Tabs>

### üß† LLM Node Tracing: Understanding AI Decisions

**What it does:** Captures every interaction with language models, including prompts, responses, and performance metrics.

**Perfect for:** GPT/Claude calls, prompt engineering, token usage monitoring, model performance analysis

**Key capabilities:**
- Complete prompt history - Every prompt and response is recorded
- Performance metrics - Response time, token usage, and costs
- Model parameters - Temperature, max tokens, and other settings
- Error handling - Failed calls and retry attempts

<Tabs items={['Python', 'JavaScript']}>
<Tabs.Tab>
```python filename="classify_intent.py"
@tracker.trace_agent_node("intent-classifier")
async def classify_intent(user_message):
    # LLM call is automatically captured with full contextAdd commentMore actions
    response = await llm.generate(f"Classify intent: {user_message}")
    return response
```
</Tabs.Tab>
<Tabs.Tab>
```javascript filename="classifyIntent.js"
const classifyIntent = traceAgentNode({
    agentNodeId: "intent-classifier",
    callback: async (userMessage) => {
        // LLM call is automatically captured with full context
        const response = await llm.generate(`Classify intent: ${userMessage}`);
        return response;
    }
});
```
</Tabs.Tab>
</Tabs>

### üõ†Ô∏è Tool Tracing: Monitoring Actions

**What it does:** Tracks every tool execution, from simple functions to complex API calls and database operations.

**Perfect for:** Custom functions, external API integrations, database queries, file processing

**Key capabilities:**
- Function monitoring - Input parameters and return values
- Execution timing - How long each operation takes
- Error tracking - Detailed error context and stack traces
- Resource usage - Memory, CPU, and network utilization

<Tabs items={['Python', 'JavaScript']}>
<Tabs.Tab>
```python filename="search_knowledge_base.py"
@tracker.trace_agent_node("knowledge-search")
async def search_knowledge_base(query):
    # Tool execution is captured with timing and results
    results = await vector_db.similarity_search(query)
    return results
```
</Tabs.Tab>
<Tabs.Tab>
```javascript filename="searchKnowledgeBase.js"
const searchKnowledgeBase = traceAgentNode({
    agentNodeId: "knowledge-search",
    callback: async (query) => {
        // Tool execution is captured with timing and results
        const results = await vectorDb.similaritySearch(query);
        return results;Add commentMore actions
    }
});
```
</Tabs.Tab>
</Tabs>

## How They Work Together

The three tracing types create a complete observability stack:

<Steps>
### Agent Tracing Creates the Foundation
Your main agent function becomes the root trace that captures the entire workflow

### LLM & Tool Tracing Add Detail
Individual operations are automatically linked as child traces under the agent

### Complete Visibility Emerges
You get a hierarchical view showing exactly how your agent processes each request
</Steps>

**Example trace hierarchy:**
```
ü§ñ Customer Support Agent (2.3s)
‚îú‚îÄ‚îÄ üß† Intent Classification (0.8s)
‚îú‚îÄ‚îÄ üõ†Ô∏è Knowledge Search (0.9s)
‚îú‚îÄ‚îÄ üß† Response Generation (0.6s)
‚îî‚îÄ‚îÄ üõ†Ô∏è Response Validation (0.1s)
```

<Callout type="success">
  **Automatic correlation:** All operations are automatically linked together, so you can see the complete story of how your agent handled each request.
</Callout>

## Why This Matters

**üîç Complete Visibility** - See every operation, understand execution flow, track data transformations

**üêõ Faster Debugging** - Pinpoint exact failure locations, access complete error context, reduce debugging time from hours to minutes

**‚ö° Performance Optimization** - Identify bottlenecks, track resource usage, optimize prompts and model parameters

**üìä Production Monitoring** - Monitor agent health, track success rates, get alerts for anomalies

## Getting Started

<Steps>
### Choose Your Starting Point
Pick the tracing type that matches your immediate needs

### Implement Tracing
Add decorators or wrappers to your agent functions

### View Results
See complete traces in your Handit.ai dashboard

### Optimize & Scale
Use insights to improve your agent's performance
</Steps>

<Callout type="tip">
  **Start simple:** Begin with Agent Tracing for your main workflow, then add LLM and Tool tracing for detailed insights. You can implement tracing incrementally without disrupting your existing code.
</Callout>

## Explore Each Feature

Ready to dive deeper into each tracing capability?

- Learn about [Agent Tracing](/tracing/tracing_features/agent) for end-to-end workflow monitoring
- Explore [LLM Node Tracing](/tracing/tracing_features/llm_node) for AI model interaction tracking
- Check [Tool Tracing](/tracing/tracing_features/tool) for function and tool execution monitoring

*Transform your AI agents from mysterious black boxes into fully observable, debuggable, and optimizable systems.*

